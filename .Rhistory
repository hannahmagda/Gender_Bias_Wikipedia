geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits (Grouped)", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(combined_data, aes(x = as.factor(Avg_Edits_Per_Month), fill = Gender)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal()
combined_data$Avg_Monthly_Edits_Grouped <- cut(combined_data$Avg_Edits_Per_Month, breaks = c(0, 5, 10, 15, 20), labels = c("0-5", "6-10", "11-15", "16-20"))
ggplot(combined_data, aes(x = as.factor(Avg_Monthly_Edits_Grouped), fill = Gender)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal()
ggplot(combined_data, aes(x = Avg_Monthly_Edits_Grouped, fill = Gender)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits (Grouped)", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(combined_data, aes(x = as.factor(Avg_Monthly_Edits_Grouped), fill = Gender)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal()
View(deu_core)
#deu_core <- head(deu_core)
deu_pol_alive <- deu_core %>% filter(is.na(death))
?legislatoR()
deu_pol <- get_political(legislature = "deu")
View(deu_pol)
View(deu_pol_alive)
###########functions
###text retreival
text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
deu_core <- get_core(legislature = "deu")
#deu_core <- head(deu_core)
deu_core_alive <- deu_core %>% filter(is.na(death))
deu_pol <- get_political(legislature = "deu")
#iterate over dataset
deu_alive_text <- deu_core_alive %>%
mutate(plain_text = sapply(wikititle, text_pipeline))
# 'plain_text' enthält nur die plain text Versionen der Wikipedia-Artikel
View(deu_alive_text)
deu_alive_text$Word_Count <- sapply(deu_alive_text$plain_text, count_words)
View(deu_alive_text)
ggplot(deu_alive_text, aes(x = Word_Count, fill = Gender)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits (Grouped)", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(deu_alive_text, aes(x = Word_Count, fill = Sex)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits (Grouped)", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(deu_alive_text, aes(x = Word_Count, fill = sex)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Average Monthly Edits (Grouped)", y = "Count", title = "Distribution of Average Monthly Edits by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(deu_alive_text, aes(x = Word_Count, fill = sex)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Word_Count", y = "Count", title = "Distribution of word count by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
setwd("~/Documents/Zukunft/Master/drittes semester/thesis/Code/Thesis")
#save the current state of scraped data
write.csv(deu_alive_text, file = "deu_alive_text")
View(deu_alive_text)
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, , 10, 15, 20), labels = c("0-100", "101-1000", "1001-3000", "3001-5000", "5001-7000" "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, , 10, 15, 20), labels = c("0-100", "101-1000", "1001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, , 10, 15, 20), labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, 100,  2000, 3000, 5000), labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, 100,  2000, 3000, 5000, 7000, 9000), labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, 100, 2000, 3000, 5000, 7000, 9000), labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- as.numeric(deu_alive_text$Word_Count)
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count, breaks = c(0, 100, 2000, 3000, 5000, 7000, 9000), labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"))
deu_alive_text$Word_Count <- cut(deu_alive_text$Word_Count,
breaks = c(0, 100, 2000, 3000, 5000, 7000, 9000, 30000),
labels = c("0-100", "101-2000", "2001-3000", "3001-5000", "5001-7000", "7001-9000", "9001-30000"),
include.lowest = TRUE)  # include.lowest to include the lower bound in the first interval
ggplot(deu_alive_text, aes(x = Word_Count, fill = sex)) +
geom_bar(stat = "count", position = "dodge") +
labs(x = "Word_Count", y = "Count", title = "Distribution of word count by Gender") +
scale_fill_manual(values = c("blue", "pink")) + # Farben anpassen
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
wp_content_ID <- page_content("es","wikipedia", page_id = "10062062", as_wikitext=TRUE)
View(wp_content_ID)
wp_content_ID[["parse"]][["wikitext"]][["*"]]
209934
View(wp_content_ID)
View(wp_content_ID)
wp_content_ID <- page_content("de","wikipedia", page_id = "10062062", as_wikitext=TRUE)
View(wp_content_ID)
wp_content_ID <- page_content("de","wikipedia", page_id = 10062062, as_wikitext=TRUE)
View(wp_content_ID)
wp_content_ID[["parse"]][["wikitext"]][["*"]]
wp_content_ID <- page_content("de", "wikipedia", page_id = 10062062
)
wp_content_ID <- page_content("de", "wikipedia", page_id = 10062062)
wp_content_ID <- page_content("de", "wikipedia", page_id = 10062062)
View(wp_content_ID)
wp_content_ID[["parse"]][["text"]][["*"]]
wp_content_ID <- page_content("es","wikipedia", page_id = "209934", as_wikitext=TRUE)
View(wp_content_ID)
wp_content_ID <- page_content("es","wikipedia", page_id = "209934")
View(wp_content_ID)
wp_content_ID <- page_content("es","wikipedia", page_id = 209934)
View(wp_content_ID)
wp_content_ID <- page_content("de","wikipedia", page_id = Q106498)
wp_content_ID <- page_content("de","wikipedia", page_id = Q106498)
View(wp_content_ID)
page_content <- wiki_page(778187)
page_content <- wiki_page("778187")
page_id <- "778187"
page_content <- wikipedia(page_id, "de")
wp_content <- page_content("en", "wikipedia", page_id = 12)
wp_content <- page_content("en", "wikipedia", page_id = 12)
View(wp_content_ID)
wp_content_ID[["parse"]][["text"]][["*"]]
?legislatoR()
fr_core <-  get_core((legislature = "fra"))
View(fr_core)
View(wp_content_ID)
wp_content_ID <- page_content("fr","wikipedia", page_id = 8665925)
wp_content_ID <- page_content("fr","wikipedia", page_id = 8665925)
View(wp_content_ID)
View(deu_core_text)
View(deu_alive_text)
View(deu_alive_text)
View(deu_history)
View(deu_pol)
library(legislatoR)
library(WikipediR)
library(rvest)
library(dplyr)
library(stringr) #für word count
library(ggplot2)
?legislator()
?legislatoR()
fr_core <-  get_core((legislature = "fra"))
fr_core_alive <- fr_core %>% filter(is.na(death))
View(fr_core_alive)
View(deu_pol_alive)
aut_core <- get_core(legislature = "aut")
aut_core_alive <- deu_core %>% filter(is.na(death))
View(aut_core_alive)
can_core <- get_core(legislature = "can")
can_core_alive <- deu_core %>% filter(is.na(death))
cze_core <- get_core(legislature = "cze")
cze_core_alive <- deu_core %>% filter(is.na(death))
View(cze_core_alive)
esp_core <- get_core(legislature = "esp")
esp_core_alive <- deu_core %>% filter(is.na(death))
irl_core <- get_core(legislature = "irl")
irl_core_alive <- deu_core %>% filter(is.na(death))
sco_core <- get_core(legislature = "sco")
sco_core_alive <- deu_core %>% filter(is.na(death))
gbr_core <- get_core(legislature = "gbr")
gbr_core_alive <- deu_core %>% filter(is.na(death))
fr_core <-  get_core((legislature = "fra"))
fr_core_alive <- fr_core %>% filter(is.na(death))
deu_core <- get_core(legislature = "deu")
deu_core_alive <- deu_core %>% filter(is.na(death))
aut_core <- get_core(legislature = "aut")
aut_core_alive <- aut_core %>% filter(is.na(death))
can_core <- get_core(legislature = "can")
can_core_alive <- can_core %>% filter(is.na(death))
cze_core <- get_core(legislature = "cze")
cze_core_alive <- cze_core %>% filter(is.na(death))
esp_core <- get_core(legislature = "esp")
esp_core_alive <- esp_core %>% filter(is.na(death))
irl_core <- get_core(legislature = "irl")
irl_core_alive <- irl_core %>% filter(is.na(death))
sco_core <- get_core(legislature = "sco")
sco_core_alive <- sco_core %>% filter(is.na(death))
gbr_core <- get_core(legislature = "gbr")
gbr_core_alive <- deu_core %>% filter(is.na(death))
use_house <- get_core(legislature = "usa", house_type = "usa_house")
use_house <- get_core(legislature = "usa_house")
use_senate <- get_core(legislature = "usa_senate")
use_core <- bind_rows(use_house, use_senate)
usa_core_alive <- usa_core %>% filter(is.na(death))
use_house <- get_core(legislature = "usa_house")
use_senate <- get_core(legislature = "usa_senate")
usa_core <- bind_rows(use_house, use_senate)
usa_core_alive <- usa_core %>% filter(is.na(death))
View(aut_core_alive)
View(can_core)
View(can_core_alive)
View(cze_core_alive)
View(cze_core_alive)
View(esp_core_alive)
aut_core_subset <- head(aut_core_alive, 5)
View(aut_core_subset)
aut_alive_text <- aut_core_alive %>%
mutate(plain_text = sapply(wikititle, text_pipeline))
aut_alive_text <- aut_core_subset %>%
mutate(plain_text = sapply(wikititle, text_pipeline))
View(aut_alive_text)
usa_core_subset <- head(usa_core_alive, 5)
usa_alive_text <- usa_core_subset %>%
mutate(plain_text = sapply(wikititle, text_pipeline))
View(usa_alive_text)
###########functions
###text aquisition german
de_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###text aquisition english
en_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("en", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
usa_alive_text <- usa_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(usa_alive_text)
gbr_core_subset <- head(usa_core_alive, 5)
gbr_core_subset <- head(gbr_core_alive, 5)
View(gbr_core_subset)
gbr_core_alive <- gbr_core %>% filter(is.na(death))
gbr_core_subset <- head(gbr_core_alive, 5)
View(gbr_core_subset)
gbr_alive_text <- gbr_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(gbr_alive_text)
wp_content <- page_content("en", "wikipedia", page_id = 12)
page_content <- wikipedia(page_id, "de")
View(wp_content)
wp_content[["parse"]][["text"]][["*"]]
wp_content <- page_content("es","wikipedia", page_name = "Abel_Matutes")
View(wp_content)
wp_content[["parse"]][["text"]][["*"]]
wp_content <- page_content("de","wikipedia", page_name = "Alfons_Pawelczyk", as_wikitext=TRUE)
View(wp_content)
wp_content[["parse"]][["wikitext"]][["*"]]
###########functions
###text aquisition german
de_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###text aquisition english
en_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
en_wp_content <- WikipediR::page_content("en", "wikipedia", page_name = page_name)
en_plain_text <- html_text(read_html(en_wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
gbr_core_subset <- head(gbr_core_alive, 5)
gbr_alive_text <- gbr_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(gbr_alive_text)
###########functions
###text aquisition german
de_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###text aquisition english
en_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
en_wp_content <- WikipediR::page_content("en", "wikipedia", page_name = page_name)
en_plain_text <- html_text(read_html(en_wp_content$parse$text$`*`))
return(en_plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
gbr_core_subset <- head(gbr_core_alive, 5)
gbr_alive_text <- gbr_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(gbr_alive_text)
###########functions
###text aquisition german
de_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###text aquisition english
en_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
en_wp_content <- WikipediR::page_content("en", "wikipedia", page_name = page_name)
en_plain_text <- html_text(read_html(en_wp_content$parse$text$`*`))
return(en_plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
gbr_core_subset <- head(gbr_core_alive, 5)
gbr_alive_text <- gbr_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(gbr_alive_text)
###########functions
###text aquisition german
de_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Check if page_name is missing
if (is.na(page_name) || page_name == "") {
return("No Wikipedia page name provided or missing.")
}
# Try fetching Wikipedia content
tryCatch({
wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
plain_text <- html_text(read_html(wp_content$parse$text$`*`))
return(plain_text)
}, error = function(e) {
return(paste("Error fetching content for page:", page_name))
})
}
###text aquisition english
en_text_pipeline <- function(page_name) {
Sys.sleep(runif(1, 1, 2))
# Überprüfen, ob der page_name fehlt
if (is.na(page_name) || page_name == "") {
return("Es wurde kein Wikipedia-Seitenname angegeben oder es fehlt.")
}
# Versuchen, Wikipedia-Inhalt abzurufen
tryCatch({
en_wp_content <- WikipediR::page_content("en", "wikipedia", page_name = page_name)
# Überprüfen, ob Inhalt verfügbar ist
if (!is.null(en_wp_content$parse$text$`*`)) {
en_html_content <- en_wp_content$parse$text$`*`
# HTML-Text in lesbar formatierten Text umwandeln
en_plain_text <- html_text(read_html(en_html_content), trim = TRUE)
return(en_plain_text)
} else {
return(paste("Kein Inhalt gefunden für Seite:", page_name))
}
}, error = function(e) {
return(paste("Fehler beim Abrufen des Inhalts für Seite:", page_name))
})
}
# Beispiel-Nutzung
result <- en_text_pipeline("Aaron_Bell_(politician)")
cat(result)
###word count
count_words <- function(text) {
words <- str_extract_all(text, "\\b\\w+\\b")[[1]]
return(length(words))
}
gbr_alive_text <- gbr_core_subset %>%
mutate(plain_text = sapply(wikititle, en_text_pipeline))
View(gbr_alive_text)
wp_content <- page_content("en","wikipedia", page_name = "Abena_Oppong-Asare")
View(wp_content)
wp_content[["parse"]][["text"]][["*"]]
install.packages("xml2")
library(xml2)
install.packages("xml2")
library(xml2)
install.packages("xml2")
# Ihr HTML-Text
html_text <- '<div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"> ... </div>'
# HTML in ein XML-Objekt umwandeln
html_doc <- read_html(html_text)
# Plain Text extrahieren
plain_text <- html_text(html_doc)
# Den resultierenden Plain Text anzeigen
cat(plain_text)
library(legislatoR)
#nur mit title# library(getwiki)
library(WikipediR)
library(rvest)
library(xml2)
# Ihr HTML-Text
html_text <- '<div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"> ... </div>'
# HTML in ein XML-Objekt umwandeln
html_doc <- read_html(html_text)
# Plain Text extrahieren
plain_text <- html_text(html_doc)
# Den resultierenden Plain Text anzeigen
cat(plain_text)
View(wp_content_ID)
# Ihr HTML-Text
html_text <- wp_content[["parse"]][["text"]]
# HTML in ein XML-Objekt umwandeln
html_doc <- read_html(html_text)
html_text <- wp_content[["parse"]][["text"]]
# Plain Text extrahieren
plain_text <- html_text(html_text)
# Den resultierenden Plain Text anzeigen
cat(plain_text)
wp_content <- page_content("en","wikipedia", page_name = "Abena_Oppong-Asare")
html_text <- wp_content[["parse"]][["text"]]
# Plain Text extrahieren
plain_text <- html_text(html_text)
View(wp_content)
