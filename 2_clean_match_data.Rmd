---
title: "Skript to clean and match the data"
author: "Hannah Schweren"
date: "2024-03-15"
output: html_document
---


This script provides the code for cleaning the data and thereby preparing it for the further analysis. Also, it includes code for the matching procedure of the data. This script outputs the two datasets used for the analysis: "Deu", containing the whole cleaned data for all German politicians and "matched_data". the dataset containing the matched body of politicians.

### Necessary packages
```{r message=FALSE, warning=FALSE}
library(legislatoR)
library(WikipediR)
library(rvest)
library(dplyr)
library(purrr)
library(tm)
library(MatchIt)
library(pbapply)
library(here)
```


### Get the raw data
```{r}
deu_political <- read.csv(here("data", "raw", "deu_political.csv"))
deu_traffic <- read.csv(here("data", "raw", "deu_traffic.csv"))
deu_text <- read.csv(here("data", "raw", "deu_text.csv"))
deu_office <- read.csv(here("data", "raw", "deu_offices.csv"))
deu_history <- read.csv(here("data", "raw", "deu_history.csv"))


```

### Necessary functions

The following code provides all the functions used in this script to clean the data, create new variables containing sub-sections of the wikipedia text, the number of links of each text and if a politician holds an important office.
```{r}

#function to clean the aquired text data

clean_data <- function(df) {
  initial_rows <- nrow(df)
  
  # Remove CSS-like structures
  #df$plain_text <- str_remove_all(df$plain_text, "\\..*?\\{.*?\\}")

  # Initialize counters for removal reasons
  removal_reason_redirect <- sum(grepl("^(Redirect to:|Weiterleitung nach:|Rediriger vers:|Redirige a:|Přesměrování na:)", df$plain_text, ignore.case = TRUE))
  removal_reason_refering_page <- sum(grepl("may refer to:|ist der Name folgender Personen:|Cette page d'homonymie répertorie différentes personnes|může být:", df$plain_text, ignore.case = TRUE))
  removal_reason_not_found <- sum(grepl("^(Error fetching content for page:|No Wikipedia page name provided or missing|Es wurde kein Wikipedia-Seitenname angegeben)", df$plain_text, ignore.case = TRUE))
  
  
  # Filter rows based on conditions
  df <- df %>%
    filter(!grepl("^(Redirect to:|Weiterleitung nach:|Rediriger vers:|Redirige a:|Přesměrování na:)", plain_text, ignore.case = TRUE) &
             !grepl("may refer to:|ist der Name folgender Personen:|Cette page d'homonymie répertorie différentes personnes|může být:", plain_text, ignore.case = TRUE) &
             !grepl("Error fetching content for page:|No Wikipedia page name provided or missing|Es wurde kein Wikipedia-Seitenname angegeben", plain_text, ignore.case = TRUE))
  
  # Calculate the number of rows removed
  rows_removed <- initial_rows - nrow(df)
  
  # Print statistics about the removal reasons
  cat("Removal reasons:\n")
  cat("  - Redirect:", removal_reason_redirect, "\n")
  cat("  - Reference Page:", removal_reason_refering_page, "\n")
  cat("  - Not Found/no name_provided:", removal_reason_not_found, "\n")
  
  
  # Create a message about the cleaning process
  cat("Cleaned data: Removed", rows_removed, "rows.\n")
  
  # Return the cleaned data frame
  return(df)
}

###################################################################################################

# function to extract "life" or "party/career" section
extract_content <- function(text, section = 1) {
  tryCatch({
    parts <- str_split(text, "\\[Bearbeiten \\| Quelltext bearbeiten\\]")[[1]]
    
    if (length(parts) > section) {
      content_between = parts[section]
      paragraph_positions <- str_locate_all(content_between, "\n\n")[[1]][,1]
      
      if (length(paragraph_positions) > 0) {
        last_paragraph_pos <- max(paragraph_positions, na.rm = TRUE)
        return(substr(content_between, 1, last_paragraph_pos))
      } else {
        return(content_between)
      }
    } else {
      return(NA)
    }
  }, error = function(e) { 
    NA 
  })
}


#############################################################################################################

#function to check, wether a politician held an important office. 
check_office <- function(wikidataid, offices_dataset) {
  office_row <- offices_dataset[offices_dataset$wikidataid == wikidataid, ]
  
  # columns indicating that a politicna held an important office
  specific_columns <- c("bundesminister", "federal_chancellor_of_germany", 
                        "president_of_germany", "president_of_the_bundestag")
  
  # plus all columns beginning with federal minister
  federal_minister_columns <- grep("^federal_minister", names(offices_dataset), value = TRUE)
  
  # combine those columns
  relevant_columns <- c(specific_columns, federal_minister_columns)
  
  # check if columns exist
  relevant_columns <- relevant_columns[relevant_columns %in% names(office_row)]
  
  # check for cases that match the condition
  any_true <- any(office_row[relevant_columns] == TRUE, na.rm = TRUE)
  
  # Return TRUE if any relevant column contains TRUE, otherwise return FALSE
  return(ifelse(length(any_true) == 0, FALSE, any_true))
}

##################################################################################################

#function to get the number of links per page

get_links <- function(wiki_name) {
  # Initialize num_links as NA
  num_links <- NA
  
  tryCatch({
    Sys.sleep(sample(1:2, 1))
    
    # Call the page_links function to get the data for the specified wiki_name
    links_data <- page_links("de", "wikipedia", limit = 300, page = wiki_name)
    
    # Check if the 'links' list exists and is not empty
    if (!is.null(links_data[["query"]][["pages"]][[1]][["links"]]) && 
        length(links_data[["query"]][["pages"]][[1]][["links"]]) > 0) {
      # Extract the number of links
      num_links <- length(links_data[["query"]][["pages"]][[1]][["links"]])
    }
  }, error = function(e) {

  })
  
  return(num_links)
}
```

### Apply the functions to the dataframe

Using the above functions, we can remove redirects and other observations containing errors in the scraped data and are left with only meaningful observations
```{r}
#clean the data
deu <- clean_data(deu_text)

```


Next, we add the variables for the extracted life and career sections for the old and current data
```{r}

#extract "life" section
deu <- deu %>%
  mutate(extracted_text = map_chr(plain_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 2)))


#extract "career" section
deu <- deu %>%
  mutate(career_text = map_chr(plain_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 3)))

#################################################################################################################

# same procedure for the old version data

#extract old "life" section
deu <- deu %>%
  mutate(extracted_text_old = map_chr(old_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 2)))


#extract old "career" section
deu <- deu %>%
  mutate(career_text_old = map_chr(old_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 3)))

#in order to only compare old texts to new ones that already existed ten years ago, I add a column that only contains current texts, if old ones exist as well.
deu$extracted_text_new <- ifelse(is.na(deu$extracted_text_old), NA, deu$extracted_text)

deu$career_text_new <- ifelse(is.na(deu$career_text_old), NA, deu$career_text)

#add a column stating the text length of the biography sections
deu <- deu %>%
  mutate(text_length_life = nchar(extracted_text))

deu <- deu %>%
  mutate(text_length_career = nchar(career_text))

deu <- deu %>%
  mutate(text_length = nchar(plain_text))



```


Then, we add more variables for further analysis, namely the page traffic per politician, the aggregated number of edits per politician and the number of links contained in each politician's text.
```{r}
#only keep oldest session if there are several sessions for one person and sum up service time
deu_political <- deu_political %>%
  group_by(pageid) %>%
  mutate(
    total_service = if (n() > 1) sum(service, na.rm = TRUE) else if_else(!is.na(service), service, 0)
  ) %>%
  slice(which.min(session)) %>%
  ungroup()


#sum of traffic data per politician
total_traffic_per_politician <- deu_traffic %>%
  group_by(pageid) %>%
  summarise(total_traffic = sum(traffic))


#######################################################################################################
#add number of links per politician

#deu$number_of_links <- pbsapply(deu$wikititle, get_links)


###########################################################################################################

#add number of edits per politician


total_edits_per_politician <- deu_history %>%
  group_by(pageid) %>%
  summarise(total_edits = n()) 

total_edits_per_politician$pageid <- as.integer(total_edits_per_politician$pageid)
deu <- left_join(deu, total_edits_per_politician, by = "pageid")




```

Next, columns for the total length of service are created in the deu dataframe by merging the information from the political dataframe of the legislatoR package. Same goes for the traffic data. Further, the birthyear of each politician is extracted and added as a variable and important offices are marked in a variable, applying the check office function from above.
```{r}

deu_political$pageid <- as.integer(deu_political$pageid)
deu <- left_join(deu, select(deu_political, pageid, session, party, total_service), by = "pageid")
total_traffic_per_politician$pageid <- as.integer(total_traffic_per_politician$pageid)
deu <- left_join(deu, select(total_traffic_per_politician, pageid, total_traffic), by = "pageid")

deu$birthyear <- substr(deu$birth, 1, 4)
deu$birthyear <- as.numeric(as.character(deu$birthyear))



#new variable to indicate if politician had an important office
deu$important_office <- sapply(deu$wikidataid, check_office, offices_dataset = deu_office)



```

### Matching procedure 

In the next step, the data is matched on the follwoing variables: birthyear, total_service, total_traffic, party, important_office, session in order to make it more comparable for the further analysis
```{r}


deu <- deu[complete.cases(deu$sex), ]
deu$sex <- ifelse(deu$sex == "male", 0, 1)

match_obj <- matchit(sex ~ birthyear + total_service + total_traffic + party + important_office + session,
                     data = deu, method = "nearest", distance = "logit",
                     exact = "session", # Exaktes Matching auf 'session'
                     ratio = 1, replace = FALSE)

#get matched data
matched_data <- match.data(match_obj)

deu$sex <- ifelse(deu$sex == 0, "male", "female")
matched_data$sex <- ifelse(matched_data$sex == 0, "male", "female")


```

### Save the clean data

Now, the data is ready to be used for the analysis and is saved for the next script
```{r}
# write.csv(deu, file = "data/clean/deu.csv", row.names = FALSE)
# write.csv(matched_data, file = "data/clean/matched_data.csv", row.names = FALSE)

```

