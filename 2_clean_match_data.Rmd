---
title: "Skript to clean and match the data"
author: "Hannah Schweren"
date: "2024-03-15"
output: html_document
---

This script provides the code for cleaning the data and thereby preparing it for the further analysis. Also, it includes the code for the matching strategy to make the data comparable. This script outputs the two data sets used for the analysis: "Deu", containing the whole cleaned data for all German politicians and "matched_data", the dataset containing the matched body of politician's articles.

### Necessary packages
```{r message=FALSE, warning=FALSE}
library(legislatoR)
library(WikipediR)
library(rvest)
library(dplyr)
library(purrr)
library(tm)
library(MatchIt)
library(pbapply)
library(here)
```


### Get the raw data
```{r}
deu_political <- read.csv(here("data", "raw", "deu_political.csv"))
deu_traffic <- read.csv(here("data", "raw", "deu_traffic.csv"))
deu_text <- read.csv(here("data", "raw", "deu_text.csv"))
deu_office <- read.csv(here("data", "raw", "deu_offices.csv"))
deu_history <- read.csv(here("data", "raw", "deu_history.csv"))


```

### Necessary functions

The following code provides all the functions used in this script to clean the data, create new variables containing sub-sections of the wikipedia text, the number of links of each text and if a politician holds an important office.
```{r}

#function to clean the aquired text data

clean_data <- function(df) {
  initial_rows <- nrow(df)

  # Initialize counters for removal reasons
  removal_reason_redirect <- sum(grepl("Weiterleitung nach:)", df$plain_text, ignore.case = TRUE))
  removal_reason_refering_page <- sum(grepl("ist der Name folgender Personen:", df$plain_text, ignore.case = TRUE))
  removal_reason_not_found <- sum(grepl("^(Es wurde kein Wikipedia-Seitenname angegeben)", df$plain_text, ignore.case = TRUE))
  
  
  # Filter rows based on conditions
  df <- df %>%
    filter(!grepl("^(Weiterleitung nach:|Rediriger vers:)", plain_text, ignore.case = TRUE) &
             !grepl("ist der Name folgender Personen:", plain_text, ignore.case = TRUE) &
             !grepl("Es wurde kein Wikipedia-Seitenname angegeben", plain_text, ignore.case = TRUE))
  
  # Calculate the number of rows removed
  rows_removed <- initial_rows - nrow(df)
  
  # Print statistics about the removal reasons
  cat("Removal reasons:\n")
  cat("  - Redirect:", removal_reason_redirect, "\n")
  cat("  - Reference Page:", removal_reason_refering_page, "\n")
  cat("  - Not Found/no name_provided:", removal_reason_not_found, "\n")
  
  
  # Create a message about the cleaning process
  cat("Cleaned data: Removed", rows_removed, "rows.\n")
  
  # Return the cleaned data frame
  return(df)
}

###################################################################################################

# function to extract "life" or "party/career" section, depending on number inputed for "section"
extract_content <- function(text, section = 1) {
  tryCatch({
    parts <- str_split(text, "\\[Bearbeiten \\| Quelltext bearbeiten\\]")[[1]]
    
    if (length(parts) > section) {
      content_between = parts[section]
      paragraph_positions <- str_locate_all(content_between, "\n\n")[[1]][,1]
      
      if (length(paragraph_positions) > 0) {
        last_paragraph_pos <- max(paragraph_positions, na.rm = TRUE)
        return(substr(content_between, 1, last_paragraph_pos))
      } else {
        return(content_between)
      }
    } else {
      return(NA)
    }
  }, error = function(e) { 
    NA 
  })
}

#############################################################################################################

#function to check, wether a politician held an important office. 
check_office <- function(wikidataid, offices_dataset) {
  office_row <- offices_dataset[offices_dataset$wikidataid == wikidataid, ]
  
  # columns indicating that a politicna held an important office
  specific_columns <- c("bundesminister", "federal_chancellor_of_germany", 
                        "president_of_germany", "president_of_the_bundestag")
  
  # plus all columns beginning with federal minister
  federal_minister_columns <- grep("^federal_minister", names(offices_dataset), value = TRUE)
  
  # combine those columns
  relevant_columns <- c(specific_columns, federal_minister_columns)
  
  # check if columns exist
  relevant_columns <- relevant_columns[relevant_columns %in% names(office_row)]
  
  # check for cases that match the condition
  any_true <- any(office_row[relevant_columns] == TRUE, na.rm = TRUE)
  
  # Return TRUE if any relevant column contains TRUE, otherwise return FALSE
  return(ifelse(length(any_true) == 0, FALSE, any_true))
}

##################################################################################################

#function to get the number of links per page

get_links <- function(wiki_name) {
  # Initialize num_links as NA
  num_links <- NA
  
  tryCatch({
    Sys.sleep(sample(1:2, 1))
    
    # Call the page_links function to get the data for the specified wiki_name
    links_data <- page_links("de", "wikipedia", limit = 300, page = wiki_name)
    
    # Check if the 'links' list exists and is not empty
    if (!is.null(links_data[["query"]][["pages"]][[1]][["links"]]) && 
        length(links_data[["query"]][["pages"]][[1]][["links"]]) > 0) {
      # Extract the number of links
      num_links <- length(links_data[["query"]][["pages"]][[1]][["links"]])
    }
  }, error = function(e) {

  })
  
  return(num_links)
}
```

### Apply the functions to the dataframe

Using the above functions, we can remove redirects and other observations containing errors in the scraped data and are left with only meaningful observations
```{r}
#clean the data
deu <- clean_data(deu_text)

```


Next, we add the variables containing the extracted life and career sections for the old and current data
```{r}

#extract "life" section
deu <- deu %>%
  mutate(extracted_text = map_chr(plain_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 2)))


#extract "career" section
deu <- deu %>%
  mutate(career_text = map_chr(plain_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 3)))

#################################################################################################################

# same procedure for the old version data

#extract old "life" section
deu <- deu %>%
  mutate(extracted_text_old = map_chr(old_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 2)))


#extract old "career" section
deu <- deu %>%
  mutate(career_text_old = map_chr(old_text, ~possibly(extract_content, otherwise = NA_character_)(.x, section = 3)))

#in order to only compare old texts to new ones that already existed ten years ago, I add a column that only contains current texts, if old ones exist as well.
deu$extracted_text_new <- ifelse(is.na(deu$extracted_text_old), NA, deu$extracted_text)

deu$career_text_new <- ifelse(is.na(deu$career_text_old), NA, deu$career_text)

#add a column stating the text length of the biography sections
deu <- deu %>%
  mutate(text_length_life = nchar(extracted_text))

deu <- deu %>%
  mutate(text_length_career = nchar(career_text))

deu <- deu %>%
  mutate(text_length = nchar(plain_text))



```


Then, we add more variables for further analysis, namely the page traffic per politician, the aggregated number of edits per politician and the number of links contained in each politician's text.
```{r}
#only keep oldest session if there are several sessions for one person and sum up service time
deu_political <- deu_political %>%
  group_by(pageid) %>%
  mutate(
    total_service = if (n() > 1) sum(service, na.rm = TRUE) else if_else(!is.na(service), service, 0)
  ) %>%
  slice(which.min(session)) %>%
  ungroup()


#sum of traffic data per politician
total_traffic_per_politician <- deu_traffic %>%
  group_by(pageid) %>%
  summarise(total_traffic = sum(traffic))


#######################################################################################################
#add number of links per politician

#deu$number_of_links <- pbsapply(deu$wikititle, get_links)


###########################################################################################################

#add number of edits per politician


total_edits_per_politician <- deu_history %>%
  group_by(pageid) %>%
  summarise(total_edits = n()) 

total_edits_per_politician$pageid <- as.integer(total_edits_per_politician$pageid)
deu <- left_join(deu, total_edits_per_politician, by = "pageid")




```

Next, columns for the total length of service are created in the deu dataframe by merging the information from the political dataframe of the legislatoR package. Same goes for the traffic data. Further, the year of birth of each politician is extracted and added as a variable and important offices are marked in a categorical variable, applying the "check office" function from above.
```{r}

#add total duration in office and page traffic to the dataset
deu_political$pageid <- as.integer(deu_political$pageid)
deu <- left_join(deu, select(deu_political, pageid, session, party, total_service), by = "pageid")
total_traffic_per_politician$pageid <- as.integer(total_traffic_per_politician$pageid)
deu <- left_join(deu, select(total_traffic_per_politician, pageid, total_traffic), by = "pageid")

#extract the year of birth
deu$birthyear <- substr(deu$birth, 1, 4)
deu$birthyear <- as.numeric(as.character(deu$birthyear))


#add variable to indicate if politician had an important office
deu$important_office <- sapply(deu$wikidataid, check_office, offices_dataset = deu_office)



```

### Matching procedure 

In the next step, the data is matched on the following variables: birthyear, total_service, total_traffic, party, important_office, session, in order to make it more comparable for the further analysis
```{r}
#conduct the matching stratgey to my data

# Remove rows with missing values in the 'sex' column
deu <- deu[complete.cases(deu$sex), ]
# Convert 'sex' column to binary (male = 0, female = 1)
deu$sex <- ifelse(deu$sex == "male", 0, 1)

# Perform matching
match_obj <- matchit(sex ~ birthyear + total_service + total_traffic + party + important_office + session,
                     data = deu, method = "nearest", distance = "logit",
                     exact = "session", # Exaktes Matching auf 'session'
                     ratio = 1, replace = FALSE)

#get matched data
matched_data <- match.data(match_obj)

# Convert 'sex' back to categorical labels (male, female)
deu$sex <- ifelse(deu$sex == 0, "male", "female")
matched_data$sex <- ifelse(matched_data$sex == 0, "male", "female")


```

### Save the clean data

Now, the data is ready to be used for the analysis and is saved for the next script
```{r}
# write.csv(deu, file = "data/clean/deu.csv", row.names = FALSE)
# write.csv(matched_data, file = "data/clean/matched_data.csv", row.names = FALSE)

```

