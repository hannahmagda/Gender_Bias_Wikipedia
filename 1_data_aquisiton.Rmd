---
title: "Sript to aquire all necessary raw data"
author: "Hannah Schweren"
date: "2024-03-15"
output: html_document
---

This script provides the code to aquire the necessary Wikipedia Data for my analyis, using the CLD database and the wikiediR package. It outputs the raw data used for the analysis, that is "deu_text". Note: The scraping of the data will take a few hours, which should be taken into account before starting the process.

### Necessary packages
```{r message=FALSE, warning=FALSE}
library(legislatoR)
library(WikipediR)
library(rvest)
library(dplyr)
library(here)
library(httr)
library(jsonlite)
library(rvest)
library(pbapply)
```

### Get the CLD Data


In order to conduct the analysis, we need the Comparative Legislator Database for Germany which we can simply access via the legislatoR package. The data is read in and saved in the following code.
```{r}
# deu_core <- get_core(legislature = "deu")
# deu_political <- get_political(legislature = "deu")
# deu_traffic <- get_traffic(legislature = "deu")
# deu_offices <- get_office(legislature = "deu")
# deu_history <- get_history(legislature = "deu")

# write.csv(deu_core, here("data", "raw", "deu_core.csv"), row.names = FALSE)
# write.csv(deu_political, here("data", "raw", "deu_political.csv"), row.names = FALSE)
# write.csv(deu_traffic, here("data", "raw", "deu_traffic.csv"), row.names = FALSE)
# write.csv(deu_offices, here("data", "raw", "deu_offices.csv"), row.names = FALSE)
# write.csv(deu_history, here("data", "raw", "deu_history.csv"), row.names = FALSE)

```


### Necessary functions

The following code contains the functions required to gather the Wikipedia texts using the politicians' page name
```{r}

#function to aquire the current wikipedia text

de_text_pipeline <- function(page_name) {
  
  #responsible web scraping
  Sys.sleep(runif(1, 1, 2))
  
  # Check if page_name is missing
  if (is.na(page_name) || page_name == "") {
    return("No Wikipedia page name provided or missing.")
  }
  
  # Try fetching Wikipedia content and return text or message in case of errors
  tryCatch({
    wp_content <- WikipediR::page_content("de", "wikipedia", page_name = page_name)
    plain_text <- html_text(read_html(wp_content$parse$text$`*`))
    return(plain_text)
  }, error = function(e) {
    return(paste("Error fetching content for page:", page_name))
  })
}

##########################################################################################################

#function to get old versions of Wikipedia pages

get_wikipedia_content_from_past <- function(wikititle) {
  #responsible web scraping
  Sys.sleep(runif(1, 1, 2))
  rvest_session <- session(url, 
                           add_headers(`From` = "hannahschweren@gmail.com", 
                                       `User-Agent` = R.Version()$version.string))
  
  ten_years_ago <- format(Sys.Date() - 3650, "%Y%m%d%H%M%S") #set date of 10 years ago
  #put together URL of the old version's text
  url <- paste0("https://de.wikipedia.org/w/api.php?action=query&prop=revisions&titles=", 
                wikititle, "&rvlimit=1&rvdir=older&rvstart=", ten_years_ago, 
                "&format=json&formatversion=2")
  
  response <- GET(url)  #Make a GET request to the URL
  response_content <- content(response, "text", encoding = "UTF-8") # Get the content of the response
  parsed_content <- fromJSON(response_content) # Parse the JSON content
  
  # Get the revision ID from the parsed JSON
  revision_id <- parsed_content$query$pages$revisions[[1]]$revid
  
  # Construct the URL to get the content of the old version
  content_url <- paste0("https://de.wikipedia.org/w/api.php?action=parse&oldid=", 
                        revision_id, "&format=json&prop=text")
  
  content_response <- GET(content_url) # Make a GET request to the content URL
  content_response_content <- content(content_response, "text", encoding = "UTF-8") # Get the content
  parsed_content_response <- fromJSON(content_response_content) # Parse the JSON content
  
  # Extract html content
  html_content <- parsed_content_response$parse$text$`*`
  html_doc <- read_html(html_content)
  
  # extract and return the plain wikipedia text
  plain_text <- html_text(html_doc)

  return(plain_text)
}

#function to apply the function for the old data while showing progress

apply_function_with_progress <- function(df) {
  df$old_text <- pblapply(df$wikititle, function(title) {
    tryCatch({
      get_wikipedia_content_from_past(title)
    }, error = function(e) {
      NA  
    })
  })
  return(df)
}



```


### Apply functions
Next, the functions are applied to the German CLD dataset in order to get the politician's biographies 
```{r}

# aquire full wiki text per politician

# deu_text <- deu_core %>%
#   mutate(plain_text = sapply(wikititle, de_text_pipeline))

###################################################################################################################

#apply functions to get old versions of wikipedia pages

# deu_text <- apply_function_with_progress(deu_text)
# 
# deu_text$old_text <- sapply(deu_text_old$old_text, function(x) {
#   if (is.list(x)) {
#     paste(unlist(x), collapse=", ")
#   } else {
#     x
#   }
# })

```




### Save the raw data

The raw data is saved in order to be used for the follwoing analysis
```{r}
#write.csv(deu_text, file = here("data", "raw", "deu_text.csv"), row.names = FALSE)

```


